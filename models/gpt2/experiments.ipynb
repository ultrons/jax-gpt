{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972f451e-7865-410b-827c-4677ffc6fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel as GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8acb77-b35b-4578-8119-bbcfdc286ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The answer to the ultimate question of life, what is a language model? The answer to the question of language is not a matter of semantics but of what is the language model. This is a question of the essence of language.\\n\\nThe language model is a matter of understanding the concepts in a language model. The understanding of the concepts is a matter of understanding the laws of the language with which the language model is based. The understanding of the laws of the language with which the language model is based is a matter of understanding the laws of the language with which the language model is based. Language models are not the same as linguistic models. They are different.\\n\\nThe relationship between language models and the laws of the language is one of understanding the concepts in a language model. The understanding of the concepts is not the same as understanding the laws of the language with which the language model is based.\\n\\nThe understanding of the concepts is not the same as understanding the laws of the language with which the language model is based.\\n\\nThe understanding of the concepts is not the same as understanding the laws of the language with which the language model is based.\\n\\nThe understanding of the concepts is not the same as understanding the laws of the language with which the language model is based.\\n\\nThe understanding of the concepts is not the same as'},\n",
       " {'generated_text': 'The answer to the ultimate question of life, what is a language model? In the second paragraph, the question of an \"ideal\" language model is addressed to the question of how to define a model of language, what it is to be a model, and whether we can define a \"model\" in terms of it.\\n\\nIn the first sentence, the model is a \"language\" model that is \"a model of language.\" In the second sentence, the model is a language model that is \"a model of data that describes a data type\" and is \"a language model that is a model of data that describes a data type.\" In the third sentence, the model is the \"language\" model which is \"a model of data that describes a data type.\" In the fourth sentence, the model is the \"data\" model which is \"a model of data that describes a data type.\"\\n\\nIn the second sentence, the data model is the \"data\" model which is \"a model of data that describes a data type.\" In the third sentence, the data model is the \"data\" model which is \"a model of data that describes a data type.\"\\n\\nFinally, in the fourth sentence, the model is the \"data\" model which is \"a model of data that describes a data type.\" In the'},\n",
       " {'generated_text': 'The answer to the ultimate question of life, what is a language model? For every English grammar, there are hundreds more that attempt to answer this question. The problem is that no one can provide exhaustive answers to these questions. In fact, it is common practice to use the most obscure and archaic words in the English language.\\n\\nThe English Language\\n\\nThe English language is a rich resource for all sorts of information. It is a rich source of information about the world, culture, history, and society. It is the most widely used language among the human race. It is the source of knowledge and information about the world, society, and the nature of space and time. It is the source of music, and the source of drama. It is the source of art. It is the source of culture and politics. It is the source of music and dance. It is the source of science and mathematics and all of the art and science related fields.\\n\\nThere is a number of well-known English words. The word \"noun\" (noun of the form \"noun\") is used to describe a person, or the world, or some part of the world. The word \"noun\" is used in the sense of \"noun or noun\" (noun), and the term \"noun\" is a'},\n",
       " {'generated_text': \"The answer to the ultimate question of life, what is a language model?\\n\\nThe answer to this question is an even more important one. Language models are a way of understanding our relationships. They help us understand what is and isn't real. They are the only way of understanding how people interact and what is and isn't real.\\n\\nThe simplest way to understand a situation is to look at the structure of the situation. It is also the only way of understanding the interaction between the two parts of the situation.\\n\\nIf we look at a situation the way we do, we see that it's not a good system of things. We see that the system is dysfunctional and there is a great deal of tension between the two parts of the situation.\\n\\nThis is a very difficult situation. The tension between the two parts of the situation is also very high. When I look at people who were living in a very rich country, they would tell me that this is a very poor country and there is great tension between the two parts of the situation.\\n\\nThis is what we see in a situation where the people are living in a very poor country.\\n\\nWhen we look at the picture of a situation in which we are living in a very rich country, we can see that it is very simple. We see that there is\"},\n",
       " {'generated_text': \"The answer to the ultimate question of life, what is a language model? A language model is one where you can give a description of what is going on in a given world, using the following techniques:\\n\\nWhen learning a language, you are forced to choose between one or two language models. You must choose between one or two languages, or they will not work for you.\\n\\nIn order to develop your knowledge of languages, you must learn a language model of its own. This is done by making a selection of the models you have chosen and then choosing one or two that are the best model for you. You can make your selections from three different models. The best model for you is the one you choose for yourself.\\n\\nYou can choose a model that is the only one that is possible for you. You don't need to memorize all of the models you have chosen.\\n\\nYou can choose a model that you want to be able to learn, but that you don't know is a model that you have already tried.\\n\\nYou can choose a model that is not available for you to learn, but that you do have.\\n\\nYou can choose a model that is not available for you to learn, but that you try to learn.\\n\\nYou can choose a model that is not available for you to\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"The answer to the ultimate question of life, what is a language model?\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe546e69-d115-4855-a41c-405ab06e31d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param: transformer.wte.weight, Shape: torch.Size([50257, 768])\n",
      "Param: transformer.wpe.weight, Shape: torch.Size([1024, 768])\n",
      "Param: transformer.h.0.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.0.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.0.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.0.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.0.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.0.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.0.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.0.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.0.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.0.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.0.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.0.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.1.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.1.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.1.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.1.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.1.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.1.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.1.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.1.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.1.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.1.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.1.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.1.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.2.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.2.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.2.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.2.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.2.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.2.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.2.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.2.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.2.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.2.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.2.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.2.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.3.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.3.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.3.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.3.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.3.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.3.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.3.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.3.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.3.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.3.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.3.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.3.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.4.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.4.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.4.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.4.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.4.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.4.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.4.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.4.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.4.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.4.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.4.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.4.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.5.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.5.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.5.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.5.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.5.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.5.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.5.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.5.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.5.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.5.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.5.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.5.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.6.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.6.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.6.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.6.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.6.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.6.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.6.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.6.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.6.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.6.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.6.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.6.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.7.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.7.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.7.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.7.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.7.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.7.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.7.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.7.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.7.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.7.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.7.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.7.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.8.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.8.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.8.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.8.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.8.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.8.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.8.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.8.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.8.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.8.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.8.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.8.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.9.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.9.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.9.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.9.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.9.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.9.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.9.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.9.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.9.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.9.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.9.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.9.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.10.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.10.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.10.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.10.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.10.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.10.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.10.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.10.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.10.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.10.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.10.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.10.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.11.ln_1.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.11.ln_1.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.11.attn.c_attn.weight, Shape: torch.Size([768, 2304])\n",
      "Param: transformer.h.11.attn.c_attn.bias, Shape: torch.Size([2304])\n",
      "Param: transformer.h.11.attn.c_proj.weight, Shape: torch.Size([768, 768])\n",
      "Param: transformer.h.11.attn.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.11.ln_2.weight, Shape: torch.Size([768])\n",
      "Param: transformer.h.11.ln_2.bias, Shape: torch.Size([768])\n",
      "Param: transformer.h.11.mlp.c_fc.weight, Shape: torch.Size([768, 3072])\n",
      "Param: transformer.h.11.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Param: transformer.h.11.mlp.c_proj.weight, Shape: torch.Size([3072, 768])\n",
      "Param: transformer.h.11.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Param: transformer.ln_f.weight, Shape: torch.Size([768])\n",
      "Param: transformer.ln_f.bias, Shape: torch.Size([768])\n",
      "Param: lm_head.weight, Shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2.from_pretrained(\"gpt2\")\n",
    "for k,v in model.state_dict().items():\n",
    "    print (f\"Param: {k}, Shape: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac2a30e-e92c-4482-8348-5ff4581f550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from Hugging Face model: gpt2\n",
      "Weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from gpt2 import GPT as gpt\n",
    "model = gpt.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c5c9c5-db86-4e44-a878-a8932cac629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: [464, 3280, 284, 262, 8713, 1808, 286, 1204, 11, 262, 6881, 11, 290, 220]\n",
      "Generating text...\n",
      "Output Tokens: [[ 464 3280  284  262 8713 1808  286 1204   11  262 6881   11  290  220\n",
      "  1849  464 1917  351  257 1285   11  287  262  471   13  632  338  257\n",
      "   737  198  464  691  257    8  198  198  198  198  198  198  198  198\n",
      "   198  198  198  198  198  198  198  198  198  198  198  198  198  198\n",
      "   198  198  198  198  198  198  198  198]]\n",
      "\n",
      "--- Generated Text ---\n",
      "The answer to the ultimate question of life, the universe, and  The problem with a week, in the U. It's a).\n",
      "The only a)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "# --- 2. Initialize the tokenizer ---\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# --- 3. Encode a prompt ---\n",
    "prompt_text = \"The answer to the ultimate question of life, the universe, and \"\n",
    "prompt_tokens = enc.encode_ordinary(prompt_text)\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "\n",
    "# --- 4. Format the input tensor ---\n",
    "# The model expects a batch dimension, so shape is (1, T)\n",
    "input_idx = jnp.array([prompt_tokens])\n",
    "\n",
    "# --- 5. Set up for generation ---\n",
    "# Create a new RNGs object specifically for generation\n",
    "rng_key = jax.random.key(42)\n",
    "rngs = nnx.Rngs(default=rng_key) # Use 'default' as generate expects it\n",
    "\n",
    "# --- 6. Generate text ---\n",
    "print(\"Generating text...\")\n",
    "# The first argument is the tokenized prompt\n",
    "# The second is the number of new tokens to generate\n",
    "output_tokens = model.generate(\n",
    "    input_idx,\n",
    "    new_tokens=50,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    "    rngs=rngs\n",
    ")\n",
    "print(f\"Output Tokens: {output_tokens}\")\n",
    "# The output includes the prompt, so we can print it all\n",
    "generated_text = enc.decode(output_tokens[0].tolist())\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d4e11-bbda-4f51-a70a-1a3f2db7b3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# 1. Set the device (use GPU if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "\n",
    "# GPT-2 doesn't have a default padding token. We'll use the end-of-sentence token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. Define the prompt\n",
    "prompt_text = \"The answer to the ultimate question of life, the universe, and\"\n",
    "\n",
    "# 4. Encode the prompt\n",
    "# The tokenizer returns a dictionary, we need the 'input_ids' tensor\n",
    "inputs = tokenizer(prompt_text, return_tensors='pt')\n",
    "input_ids = inputs.input_ids.to(device)\n",
    "\n",
    "# 5. Generate text\n",
    "# We use similar parameters to your JAX implementation for a fair comparison\n",
    "# Note: do_sample=True is crucial to enable temperature and top_k sampling\n",
    "print(\"Generating text with Hugging Face model...\")\n",
    "output_sequences = model.generate(\n",
    "    #input_ids=input_ids,\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id # Suppress a warning\n",
    ")\n",
    "\n",
    "# 6. Decode the output\n",
    "# The output contains the prompt, so we decode the whole sequence\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Hugging Face Generated Text ---\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5704a4-de53-48f6-8bd3-dee47ecb4f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
